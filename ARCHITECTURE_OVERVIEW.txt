POKÉMON RED RL AGENT – ARCHITECTURE OVERVIEW
===========================================

Pipeline Summary
----------------
1. pyboy-powered `PokemonRedEnv` instances expose GB frames, map ids, and rich info dicts. Each env
   hard-resets from power-on, deletes the `.sav`, clears visit stats, and emits both raw badge bits
   (`0xD356`) and champion flag bit (event flag 1 at `0xD867`). Baseline badge/champion masks are
   recorded once ROM boot completes to prevent false positives.
2. `EpsilonEnv` wraps the env plus a stack of modular reward shapers (exploration, novelty, quests,
   Bayesian progress penalties, latent events, stay penalties, resource management). The wrapper
   also enforces `boot_steps`, action spacing, and no-input timeouts before forwarding observations.
3. Parallel environments (default 6, production 8) feed their observations into `SimpleDQN`, a
   recurrent dueling quantile network. Envs can render in headless SDL-null mode for cluster runs.
4. `ReplayBuffer` stores observation tensors, map features, **goal-conditioning features**, n-step
   rollouts for prioritized sampling, plus hidden-state tuples so sequence chunks can be replayed.
5. Training optimizes quantile-regression TD loss + auxiliary map reconstruction + novelty critic
   loss. A Bayesian progress tracker logs badge/flag/elite completion probabilities after each run
   and flushes summaries every 10 k global steps so long runs still yield intermediate metrics.
6. After training the runner automatically demos the best checkpoint, reusing the same rendering
   stack and printing the Bayesian progress summary. Autosaves persist `dqn_route1_latest.pt`
   every five minutes so preempted jobs can resume from recent weights.

Observation & Feature Encoding
------------------------------
- Visual encoder: 72×80 grayscale frames → stem conv stack (5×5 stride2, 3×3 stride2) feeding
  three squeeze-and-excite residual blocks and a 4-head spatial attention layer.
- Map features: `extract_map_features` builds a 32+ dimensional vector (coords, region type,
  badge progress, HP ratios, novelty stats). A lightweight MLP (`map_net`) projects these into a
  128-D embedding; further FiLM parameters (γ, β) are produced from the goal encoder to modulate it.
- **Return-conditioned goal features**: `build_map_and_goal_features` appends four scalars derived
  from progressing the storyline—(1) badge gap, (2) story flag ratio, (3) elite progress, (4)
  champion flag. A dedicated encoder (2×Linear+SiLU) and FiLM-style gamma/beta layers gate the map
  embedding so the policy can aim for high-value goals instead of farming local rewards. Goal feats
  are also fed directly to the novelty critic to keep latent space aligned with long-horizon plans.

Network Heads
-------------
- Recurrent core: concatenated GRU(384) + LSTM(512) outputs a 896-D hidden vector per step.
- Dueling distributional head: NoisyLinear → ReLU → NoisyLinear produces action-quantile advantages;
  another branch predicts state quantiles. Their combination yields a 51-atom distribution per action.
- **High-value advantage head**: an extra NoisyLinear→ReLU→Linear branch estimates per-action bonuses
  emphasising high-upside trajectories. Its logits are added to the quantile outputs, nudging the
  policy away from grinding small rewards.
- Auxiliary decoder: reconstructs the raw map feature vector (stabilises representation learning).
- **Cross-episode novelty critic**: a 128-D projection trained to match the goal feature vector,
  encouraging the hidden state to encode long-horizon progress cues used by the Bayesian tracker.

Losses
------
`loss = TD_quantile + auxiliary_loss_coef * map_recon + novelty_loss_coef * goal_alignment`
- Quantile TD loss uses prioritized replay with factorised NoisyNets noise resets per batch.
- Map reconstruction teaches the latent to stay grounded in spatial context.
- Novelty critic aligns the latent with goal features, shaping it toward unexplored, high-value
  objectives by penalizing disagreement between the critic’s 128-D projection and the target goals.
  When `novelty_loss_coef` is zero this term is disabled, but the projection head still runs so it
  can be re-enabled without retraining from scratch.

Bayesian Progress Tracking
--------------------------
- `progress_tracking.py` defines `ProgressEvent` records (badges, Elite Four, custom flags) with Beta
  priors, step limits, and decision thresholds.
- During training each env episode marks events as soon as the info dict reports completion within
  the step budget. Posterior alpha/beta values are saved to `checkpoints/progress_metrics.json` and
  summarised (mean, 95% CI, pursue/defer decision).
- Replay/demo commands print this summary before loading a checkpoint, so you can judge the policy’s
  likelihood of clearing key milestones within the allotted steps.

Post-Training Demo Flow
-----------------------
1. `train()` saves `dqn_route1_best.pt`/`dqn_route1_latest.pt` plus the Bayesian metrics file.
2. If `auto_demo_after_training` (default true) is enabled, the runner calls `run_replay` with the
   best available checkpoint, renders only the aggregate exploration map (unless `--gameplay-grid`
   is requested), and optionally saves a PNG snapshot.
3. `--watch-only` and `--watch-after-training` share the same replay stack, so you can inspect
   demos later or immediately without extra scripting.

Exploration-Focused Enhancements Recap
--------------------------------------
- Return-conditioned FiLM gating steers the agent toward badge/story completion instead of looping.
- High-value advantage bias rewards plans with greater upside.
- Novelty critic + Bayesian tracker provide quantitative feedback on whether long-horizon goals are
  being met, and drive the auxiliary loss to encode those goals directly in the latent state.
- Replay snapshots retain per-env hidden states so recurrent credit assignment stays stable even
  when sampling out-of-order n-step chunks.
- Visit-tracking, map-stay penalties, and resource rewards are modular, so experiments can disable
  individual shapers through the JSON config without touching the training core.
- **Global+episodic visit counts** now hash `(map_id, x_bin, y_bin, story_hash)` cells so Route 1
  tiles decay to near-zero intrinsic value while fresh areas remain rewarding. Optional per-episode
  first-visit bonuses keep rolls exploratory without letting a single tile farm dominate.
- **Map-transition novelty** grants a short-lived bonus the first few times the agent crosses a
  `(map_from → map_to)` boundary, incentivising door/warp traversal without scripting quests.
- **Episodic latent memory** tracks recently-seen latent vectors and emits a reward when the
  current embedding is far from the episode buffer, punishing tiny wiggle loops.
- **Random Network Distillation** runs on the SimpleDQN latent (exposed through `LowLevelDQNPolicy`)
  so distant regions stay intrinsically interesting; a running normaliser and annealed scale make
  the bonus large early and gently fade it once the agent routinely reaches Viridian/Forest.
- **State archive + frontier resets** capture savestates for new coarse cells and occasionally reset
  envs straight into them. This Go-Explore style loop lets training rehearse mid-/late-game states
  without manual scripting—ideal for long quests like Oak’s parcel chain.
- **Low-level policy wrapper** (`LowLevelDQNPolicy`) exposes goal hooks needed for HIRO-style
  managers: the backbone already returns latents + hidden states each step, so plugging in a
  high-level controller that outputs latent goals becomes a surgical change rather than a rewrite.
