SimpleDQN (with optional lightweight SSM arm)

1) Visual Stem  
   - `simple_dqn.py` → two strided 2D convolutions (C→64→160) with BatchNorm + ReLU compress the 72×80 grayscale frame.

2) Residual Encoding Stack  
   - Three `ResidualBlock`s with squeeze-and-excite keep the spatial grid while deepening features (160 channels throughout).

3) Spatial Attention  
   - `SpatialAttention` flattens the feature map, applies 4-head self-attention, and adds it residually to capture long-range spatial context.

4) Post-Attention Projection  
   - 1×1 conv + BatchNorm + ReLU keeps 160 channels, then global average pooling turns the frame into a 160-D vector. Dropout(0.1) regularises it.

5) Map / Goal Encoders  
   - `map_net`: two-layer MLP with LayerNorm to embed handcrafted map features into 72-D.  
   - `goal_encoder`: single-layer MLP to embed goal features into 36-D plus FiLM-style gamma/beta that modulate the map embedding.

6) Optional Lightweight SSM Side-Encoder (config-gated)  
   - Controlled by training_config: `use_ssm_encoder`, `ssm_state_dim`, `ssm_head_dim`, `ssm_heads`, `ssm_layers`.  
   - A small stacked selective-SSM: s_t = σ(Ax) * s_{t-1} + Bx, y = C s_t. Heads follow a multi-value pattern (shared B/C per head).  
   - Takes the fused flat features [visual | map] and emits a compact context vector (heads × head_dim) that is concatenated to the RNN input.  
   - Defaults are tiny (state_dim=32, head_dim=32, heads=2, layers=1) to stay fast on local Macs; disable via `use_ssm_encoder: false`.

7) Temporal Fusion (GRU + LSTM)  
   - Two parallel recurrent blocks on the fused vector (and SSM output when enabled): GRU hidden 208, LSTM hidden 192. Outputs are concatenated.

8) Dueling Distributional Head with NoisyNets  
   - Advantage/value streams (NoisyLinear + ReLU) predict 51 quantiles per action and combine into Q; an extra “high value” head and auxiliary map recon head plus a novelty head share the fused recurrent features.

9) Exploration + Aux Losses  
   - NoisyLinear layers encourage exploration; auxiliary map reconstruction and novelty heads are weighted by `auxiliary_loss_coef` / `novelty_loss_coef` during training.
