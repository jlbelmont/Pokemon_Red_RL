### Model Architecture & Dynamics
- Dueling distributional DQN with shared CNN encoder feeding 3 lightweight SSM heads, followed by GRU/LSTM cores (256 hidden units each) and distributional value/advantage heads.
- Intrinsic stack: visit-count bonus + optional posterior-driven RND head. RND base scale set to 0.3 and remapped via Beta posteriors of "Oak Parcel Assigned" (scale 0.2 when confident, 0.8 when posterior near 0).
- Curriculum mix ensures 4 envs cycle across fresh boots, parcel-prep savestates, and deep checkpoints (Route 2, Viridian Forest, Pewter).
- Episode limit 6k steps; state archive reset probability 0.35 keeps env restarts randomised.

### Posterior-driven RND Mix
- Core idea: shrink intrinsic novelty once the parcel posterior grows, forcing the agent to seek extrinsic parcel/Boulder rewards.
- Implementation: enable `posterior_rnd_enabled` in `training_config.json`, target event `Oak Parcel Assigned`, bounds `[0.2, 0.8]`. Each episode, `minimal_epsilon_setup.py` loads `progress_metrics.json`, reads the current Beta mean, and interpolates the RND scale.
- Until the parcel posterior exceeds ~0.2, envs receive near-max novelty (0.8). Once delivery becomes reliable, novelty decays to 0.2 so training time shifts toward Pewter/Boulder progress.
